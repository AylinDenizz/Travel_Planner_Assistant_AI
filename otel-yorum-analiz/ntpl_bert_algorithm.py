import pandas as pd
from datasets import Dataset
from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import evaluate
import re
import torch

df=pd.read_csv(r'C:\Users\aylin\OneDrive\Masa√ºst√º\travelplanner\otel-yorum-analiz\data\Hotel_Reviews.csv')

df['Negative_Review'] = df['Negative_Review'].replace("No Negative", "")
df['Positive_Review'] = df['Positive_Review'].replace("No Positive", "")

df['Full_Review'] = df['Positive_Review'] + " " + df['Negative_Review']

df_cleaned = df[['Hotel_Name', 'Hotel_Address', 'Reviewer_Score', 'Full_Review', 'lat', 'lng']].copy()


# 4. Text normalize: k√º√ß√ºk harf, noktalama temizliƒüi
def clean_text(text):
    text = str(text).lower()                        # k√º√ß√ºk harfe √ßevir
    text = re.sub(r'[^\w\s]', '', text)             # noktalama i≈üaretlerini kaldƒ±r
    text = re.sub(r'\s+', ' ', text).strip()        # fazla bo≈üluklarƒ± temizle
    return text

def classify_score(score):
    if score <= 4.0:
        return 0   # Olumsuz
    elif score <= 7.0:
        return 1   # N√∂tr
    else:
        return 2   # Olumlu

df_cleaned['Full_Review'] = df_cleaned['Full_Review'].apply(clean_text)

df_cleaned['Label'] = df_cleaned['Reviewer_Score'].apply(classify_score)

df_cleaned.to_csv("cleaned_reviews.csv", index=False)

# üß™ Adƒ±m 2: Veriyi HuggingFace Dataset formatƒ±na √ßevir
dataset = Dataset.from_pandas(df_cleaned[['Full_Review', 'Label']])

# k√º√ß√ºk bir subset ile deneyebilirsin
dataset = dataset.select(range(20000))  # ƒ∞lk 20K √∂rnekle √ßalƒ±≈ü

# ‚úÇÔ∏è Adƒ±m 3: Train/Validation ayƒ±r
dataset = dataset.train_test_split(test_size=0.2)
train_dataset = dataset['train']
val_dataset = dataset['test']

# Tokenizer y√ºkle
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize_function(example):
    return tokenizer(example["Full_Review"], padding="max_length", truncation=True, max_length=256)

train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

# Label'ƒ± modelin anlayacaƒüƒ± ≈üekilde ayarla
train_dataset = train_dataset.rename_column("Label", "labels")
val_dataset = val_dataset.rename_column("Label", "labels")

train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
val_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# üß† Adƒ±m 5: Modeli y√ºkle
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# ‚öôÔ∏è Adƒ±m 6: Eƒüitim parametreleri
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    greater_is_better=True,
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    logging_dir="./logs",
    logging_steps=10,
    report_to="none",  # üîß Kayƒ±t kriteri
    fp16=torch.cuda.is_available()      # üîß GPU varsa mixed precision kullan
)



# metric hesaplama
def compute_metrics(p):
    preds = p.predictions.argmax(-1)
    labels = p.label_ids
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average="weighted"),
        "precision": precision_score(labels, preds, average="weighted"),
        "recall": recall_score(labels, preds, average="weighted"),
    }



trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

# üöÄ Adƒ±m 9: Eƒüitimi ba≈ülat
trainer.train()

# üíæ Modeli kaydet
model.save_pretrained("saved_model")
tokenizer.save_pretrained("saved_model")


# üìä Sonu√ßlarƒ± g√∂ster
eval_result = trainer.evaluate()
print("Evaluation Results:", eval_result)


from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Model ve tokenizer'ƒ± y√ºkle
model = BertForSequenceClassification.from_pretrained("saved_model")
tokenizer = BertTokenizer.from_pretrained("saved_model")

# √ñrnek yorum
text = "The hotel room was clean and the staff were friendly."

# Tokenize et
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)

# Tahmin yap
with torch.no_grad():
    outputs = model(**inputs)
    predicted_class = torch.argmax(outputs.logits).item()

# Sonucu yazdƒ±r
print(f"Predicted class: {predicted_class}")


from sklearn.metrics import classification_report

#T√ºm test verisi √ºzerinde tahmin yap, ger√ßek ve tahmin edilenleri kar≈üƒ±la≈ütƒ±r
#classification_report(y_true, y_pred, target_names=["neg", "neutral", "pos"])
#üéâ Hangisini yapalƒ±m? üîç Tahmin √∂rneƒüi mi?

#üìä Test ve metrik analizi mi?

#üåê Web servisi mi?

#üß™ Test verisiyle batch deƒüerlendirme mi?

#Sen karar ver, ben buradayƒ±m üí™